
<html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<title>PairCustomization</title>
<link href="./style.css" rel="stylesheet">
<!-- <script type="text/javascript" src="./DreamBooth_files/jquery.js"></script> -->
</head>

<body>
<div class="content">
  <h1><strong>Customizing Text-to-Image Models with a Single Image Pair</strong></h1>
  <!-- <h2 style="text-align:center;">ECCV 2024???</h2> -->

<!--   <img src="./images/teaser.png" class="teaser-gif" style="width:100%;">	<font size="+2"> -->
  <div class="video-wrapper">
    <center>
        <video id="teaser" autoplay muted loop playsinline  width="100%">
            <source src="./images/PairCustomizationFixedBackground.mp4" type="video/mp4">
    </center>
  </div>
          <p style="text-align: center;">
            <a href="https://arxiv.org/abs/2405.01536" target="_blank">[Paper]</a> &nbsp;&nbsp;&nbsp;&nbsp;
	        <a href="https://github.com/PairCustomization/PairCustomization" target="_blank">[Code]</a>
          </p>
    </font>
    <h3 style="text-align:center;">Pair Customization customizes a diffusion model to learn stylistic difference given <u>a single image pair</u>. <br>
        Style image credits: <a href="https://www.instagram.com/parkhouse_art/">Jack Parkhouse</a></h3>
</div>


<div class="content">
  <h2 style="text-align:center;">Abstract</h2>
  <p>Art reinterpretation is the practice of creating a variation of a reference work, making a paired artwork that exhibits a distinct artistic style. 
    We ask if such an image pair can be used to customize a generative model to capture the demonstrated stylistic difference.
    We propose Pair Customization, a new customization method that learns stylistic difference from a <u>single</u> image pair and then applies the acquired style to the generation process.
    Unlike existing methods that learn to mimic a single subject from a collection of images, our method captures the stylistic difference between paired images. This allows us to apply a stylistic change without overfitting to the specific image content in the examples. To address this new task, we employ a joint optimization method that explicitly separates the style and content into distinct LoRA weight spaces. We optimize these style and content weights to reproduce the content and style images while encouraging their orthogonality. During inference, we modify the diffusion process via a new style guidance based on our learned weights. 
    Both qualitative and quantitative experiments show that our method can effectively learn style while avoiding overfitting to image content, highlighting the potential of modeling such stylistic differences from a single image pair. </p>
</div>


<div class="content">
    <h2>Problem Statement</h2>
    <p> Prior works on model customization/personalization take one or a few images of a single concept to customize large-scale text-to-image models. While they aim to learn styles without using pairs, the generated samples from these customized models often resemble the training images' content, such as specific objects, persons, and scene layouts.
        These issues arise because the artistic intent is difficult to discern from a single image: unlike image pairs that can demonstrate a style through contrasts, a singleton example will always intertwine choices of both style and content. This ambiguity can cause the model to fail to capture the artistic style accurately and overfit to the example rather than the style, especially when generating images with the same/similar prompts as the training images. 
        On the other hand, Pair Customization exploits the contrast between image pairs to better disentangling content and style.</p>
    <br>
    <img class="summary-img" src="./images/problem_statement.png" style="width:100%;"> <br>
  </div>

<div class="content">
  <h2>Pair Customization Training/Inference</h2>
  <br>
  <img class="summary-img" src="./images/method.png" style="width:100%;"> <br>
  <br>
  <p>(Left) We disentangle content and style from an image pair by jointly training two  <a href="https://huggingface.co/blog/lora">low-rank adapters</a>, ContentLoRA and StyleLoRA representing content and style, respectively. We consider the following joint finetuning procedure: (Top Row) We fine-tune ContentLoRA to reconstruct content image conditioned on a content prompt. (Bottom Row) We reconstruct the style image using <i>both</i> ContentLoRA and StyleLoRA conditioned on a style prompt, but we only optimize StyleLoRA. (Right) At inference time, we only apply StyleLoRA to customize the model. Given the same noise seed, the customized model generates a stylized counterpart of the original pre-trained model output. V* is a fixed random rare token that is a prompt modifier for the content image. Style image credits: <a href="https://www.instagram.com/parkhouse_art/">Jack Parkhouse</a> </p>
</div>


<div id="results" class="content">
  <h2>Results/Comparison with Baselines</h2>
  <p>Our method faithfully generates images with stylization applied in many settings/contexts. We compare to the two strongest baselines we tested against, or Conpcepts Sliders <a href="#references">[1]</a> and DreamBooth LoRA <a href="#references">[4, 5]</a>: </b>  </p>
  <img class="summary-img" src="./images/main_results_v7.png" style="width:100%;">
  <!-- <br>
    <img class="summary-img" src="./images/results/for_web_result2.jpg" style="width:100%;">
    <hr>
  <p>Our method works well with style transfer from real reference images while not requiring training or model personalization.  </p>
  <img class="summary-img" src="./images/results/reals2.jpg" style="width:100%;"> -->
</div>

<div id="results" class="content">
    <h2>Updated Style Application</h2>
    <p>We introduce <b>Style Guidance</b>, a new formulation for smoothly applying a stylized model to generated outputs. We find that this method is superior to LoRA scale, or the 
    practice of interpolating the strength of the output from finetuned LoRA layers. Refer to Section 3.3 of the paper for more details.</p>
    <img class="summary-img" src="./images/style_ablation.png" style="width:100%;">
    <!-- <br>
      <img class="summary-img" src="./images/results/for_web_result2.jpg" style="width:100%;">
      <hr>
    <p>Our method works well with style transfer from real reference images while not requiring training or model personalization.  </p>
    <img class="summary-img" src="./images/results/reals2.jpg" style="width:100%;"> -->
  </div>

<div id="results" class="content">
    <h2>Multi-Slider Compositionality</h2>
    <p>We can compose multiple customized models by directly blending each style guidance together. Adjusting blending strength
        of each model allows us to acquire a smooth style transition</b>  </p>
    <img class="summary-img" src="./images/blending_v2.png" style="width:100%;">
    <!-- <br>
      <img class="summary-img" src="./images/results/for_web_result2.jpg" style="width:100%;">
      <hr>
    <p>Our method works well with style transfer from real reference images while not requiring training or model personalization.  </p>
    <img class="summary-img" src="./images/results/reals2.jpg" style="width:100%;"> -->
  </div>

<!-- <div id="results2" class="content">
  <h2>Integration with Other Methods</h2>
  <p>StyleAligned can be easily combined with other methods.</p>

    <h4><a href="https://github.com/lllyasviel/ControlNet" target="_blank">ControlNet </a> + StyleAligned</h4>

    <img class="summary-img" src="./images/results/controlnet1.jpg" style="width:100%;">
    <br> <br>
    <img class="summary-img" src="./images/results/controlnet2.jpg" style="width:100%;">
    <br>
    <hr>
    <h4> <a href="https://dreambooth.github.io/" target="_blank">DreamBooth</a> + StyleAligned</h4>
    <img class="summary-img" src="./images/results/db.jpg" style="width:100%;">
    <br>
    <hr>
    <h4> <a href="https://multidiffusion.github.io/" target="_blank">MultiDiffusion</a> + StyleAligned</h4>
    <img class="summary-img" src="./images/results/multidiffusion.jpg" style="width:100%;">
</div> -->


<!-- <div class="content">
  <h2>BibTex</h2>
    <code>@article{jones2024customizing,
  title={Customizing Text-to-Image Models with a Single Image Pair},
  author={Jones, Maxwell and Wang, Sheng-Yu and Kumari, Nupur and Bau, David and Zhu, Jun-Yan},
  journal={arXiv preprint arXiv:2405.01536},
  year={2024}
}
}</code>
</div> -->

<div class="content" id="references">
    <h2>References</h2>
      1. Gandikota, R., Materzynska, J., Zhou, T., Torralba, A., Bau, D.: Concept
      sliders: Lora adaptors for precise control in diffusion models. arXiv preprint
      arXiv:2311.12092 (2023) <br>
      2. Kumari, N., Zhang, B., Zhang, R., Shechtman, E., Zhu, J.Y.: Multi-concept cus-
      tomization of text-to-image diffusion. In: 2023 IEEE/CVF Conference on Com-
      puter Vision and Pattern Recognition (CVPR). pp. 1931-1941. IEEE Computer
      Society (2023)
      3. Gal, R., Alaluf, Y., Atzmon, Y., Patashnik, O., Bermano, A.H., Chechik, G.,
      Cohen-or, D.: An image is worth one word: Personalizing text-to-image generation
      using textual inversion. In: The Eleventh International Conference on Learning
      Representations (2022)
      4. Ruiz, N., Li, Y., Jampani, V., Pritch, Y., Rubinstein, M., Aberman, K.: Dream-
      booth: Fine tuning text-to-image diffusion models for subject-driven generation.
      In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
      Recognition. pp. 22500-22510 (2023)
      5. Ryu, S.: Low-rank adaptation for fast text-to-image diffusion fine-tuning. https:
      //github.com/cloneofsimo/lora (2023)  

    <!-- <code> @article{hertz2023StyleAligned,<br>
    &nbsp;&nbsp;title={Style Aligned Image Generation via Shared Attention},<br>
    &nbsp;&nbsp;author={Hertz, Amir and Voynov, Andrey and Fruchter, Shlomi and Cohen-Or, Daniel},<br>
    &nbsp;&nbsp;booktitle={arXiv preprint arxiv:2312.02133},<br>
    &nbsp;&nbsp;year={2023}<br>
    } </code>  -->
  </div>

</body>
<div class="content" id="acknowledgements">
  <p><strong>Acknowledgements</strong>:
    The website template is taken from the <a href="https://style-aligned-gen.github.io/">Style Aligned</a> Project Page. 
  </p>
</div>
<br>
<!-- <body font-family="Google Sans">
    <h2 style="text-align:center;">Can you guess all of the places? Click on image to know the answer.</h2>
    <p style="text-align:center;">All of the images are generated with StyleAligned, first image serves as the style source.</p>
    <div id="grid-container" class="grid-container">
        <!-- Grid items will be inserted here by script.js -->
    <!-- </div>
    <script src="quiz.js"></script>
</body> --> -->

</html>
